{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b2e0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10bdc4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df_usual = pd.read_csv(\"cleaned_data_ready_for_ml.csv\")\n",
    "\n",
    "# Drop work_year (we're not using it to predict salary)\n",
    "X = df_usual.drop(['salary_in_usd', 'work_year'], axis=1)\n",
    "y = df_usual['salary_in_usd']\n",
    "\n",
    "# First, split into train (80%) and temp (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then split temp into validation (10%) and test (10%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be513bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual salary: 184000\n",
      "Predicted salary (Random Forest): 166657.5024889895\n",
      "\n",
      " Random Forest Metrics:\n",
      "MAE: 46033.571243311184\n",
      "MSE: 3323258035.0300984\n",
      "RMSE: 57647.70624257394\n",
      "R² Score: 0.21536931941493798\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict one sample\n",
    "sample_row = X_val.iloc[[0]]\n",
    "actual_salary = y_val.iloc[0]\n",
    "predicted_salary = rf_model.predict(sample_row)[0]\n",
    "\n",
    "print(\"Actual salary:\", actual_salary)\n",
    "print(\"Predicted salary (Random Forest):\", predicted_salary)\n",
    "\n",
    "# Predict full validation set\n",
    "rf_preds = rf_model.predict(X_val)\n",
    "\n",
    "# Metrics\n",
    "print(\"\\n Random Forest Metrics:\")\n",
    "print(\"MAE:\", mean_absolute_error(y_val, rf_preds))\n",
    "print(\"MSE:\", mean_squared_error(y_val, rf_preds))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_val, rf_preds)))\n",
    "print(\"R² Score:\", r2_score(y_val, rf_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e8ec49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 25\n",
      "[LightGBM] [Info] Number of data points in the train set: 34640, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 145360.840300\n",
      "Actual salary: 184000\n",
      "Predicted salary (LightGBM): 166559.7620873307\n",
      "\n",
      " LightGBM Metrics:\n",
      "MAE: 46035.0834382692\n",
      "MSE: 3321665962.9425597\n",
      "RMSE: 57633.89595492014\n",
      "R² Score: 0.21574521216609321\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "sample_row = X_val.iloc[[0]]\n",
    "actual_salary = y_val.iloc[0]\n",
    "predicted_salary = lgb_model.predict(sample_row)[0]\n",
    "\n",
    "print(\"Actual salary:\", actual_salary)\n",
    "print(\"Predicted salary (LightGBM):\", predicted_salary)\n",
    "\n",
    "# Predict full validation set\n",
    "lgb_preds = lgb_model.predict(X_val)\n",
    "\n",
    "# Metrics\n",
    "print(\"\\n LightGBM Metrics:\")\n",
    "print(\"MAE:\", mean_absolute_error(y_val, lgb_preds))\n",
    "print(\"MSE:\", mean_squared_error(y_val, lgb_preds))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_val, lgb_preds)))\n",
    "print(\"R² Score:\", r2_score(y_val, lgb_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "835e35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Correct Predictions: 0\n",
      "LightGBM Incorrect Predictions: 4330\n",
      "Random Forest Correct Predictions: 0\n",
      "Random Forest Incorrect Predictions: 4330\n"
     ]
    }
   ],
   "source": [
    "# Convert prediction to integer and calculate correct_count\n",
    "def get_correct_count(model, X, y):\n",
    "    # Predict using the model\n",
    "    preds = model.predict(X)\n",
    "    # Convert predictions to integers (round them to the nearest integer)\n",
    "    preds_int = [round(pred) for pred in preds]\n",
    "    \n",
    "    # Compare the predictions to actual values and count correct/incorrect\n",
    "    correct_count = sum([1 for i in range(len(preds_int)) if preds_int[i] == y.iloc[i]])\n",
    "    incorrect_count = len(preds_int) - correct_count\n",
    "    \n",
    "    return correct_count, incorrect_count\n",
    "\n",
    "# Example for LightGBM\n",
    "lgb_correct_count, lgb_incorrect_count = get_correct_count(lgb_model, X_val, y_val)\n",
    "print(f\"LightGBM Correct Predictions: {lgb_correct_count}\")\n",
    "print(f\"LightGBM Incorrect Predictions: {lgb_incorrect_count}\")\n",
    "\n",
    "# Example for Random Forest\n",
    "rf_correct_count, rf_incorrect_count = get_correct_count(rf_model, X_val, y_val)\n",
    "print(f\"Random Forest Correct Predictions: {rf_correct_count}\")\n",
    "print(f\"Random Forest Incorrect Predictions: {rf_incorrect_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11df8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to Calculate the performance metrics and work on improving the hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
