{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Salary Prediction using Regression\n",
    "This notebook predicts exact salary values using advanced regression techniques and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "df = pd.read_csv('salaries.csv')\n",
    "\n",
    "# Display initial statistics\n",
    "print(\"Salary Statistics (USD):\")\n",
    "print(df['salary_in_usd'].describe())\n",
    "\n",
    "# Plot salary distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['salary_in_usd'], bins=50)\n",
    "plt.title('Salary Distribution')\n",
    "plt.xlabel('Salary (USD)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Plot log-transformed salary distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(np.log1p(df['salary_in_usd']), bins=50)\n",
    "plt.title('Log-Transformed Salary Distribution')\n",
    "plt.xlabel('Log Salary')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Feature Engineering\n",
    "\n",
    "# 1. Process job titles\n",
    "def extract_job_features(title):\n",
    "    title = title.lower()\n",
    "    \n",
    "    # Seniority scoring\n",
    "    seniority_levels = {\n",
    "        'principal': 5.0,\n",
    "        'staff': 4.5,\n",
    "        'senior': 4.0,\n",
    "        'lead': 4.0,\n",
    "        'mid': 3.0,\n",
    "        'junior': 2.0,\n",
    "        'entry': 1.0\n",
    "    }\n",
    "    seniority = 3.0  # default mid-level\n",
    "    for level, score in seniority_levels.items():\n",
    "        if level in title:\n",
    "            seniority = score\n",
    "            break\n",
    "    \n",
    "    # Role complexity scoring\n",
    "    complexity_factors = {\n",
    "        'architect': 1.5,\n",
    "        'machine learning': 1.4,\n",
    "        'data scientist': 1.4,\n",
    "        'security': 1.3,\n",
    "        'devops': 1.3,\n",
    "        'full stack': 1.2,\n",
    "        'backend': 1.2,\n",
    "        'frontend': 1.1\n",
    "    }\n",
    "    complexity = 1.0\n",
    "    for factor, score in complexity_factors.items():\n",
    "        if factor in title:\n",
    "            complexity = score\n",
    "            break\n",
    "            \n",
    "    return pd.Series({\n",
    "        'seniority': seniority,\n",
    "        'complexity': complexity,\n",
    "        'job_value': seniority * complexity\n",
    "    })\n",
    "\n",
    "# Extract job features\n",
    "job_features = df['job_title'].apply(extract_job_features)\n",
    "df = pd.concat([df, job_features], axis=1)\n",
    "\n",
    "# 2. Location-based features\n",
    "location_stats = df.groupby('company_location').agg({\n",
    "    'salary_in_usd': ['mean', 'median', 'std', 'count']\n",
    "}).fillna(0)\n",
    "\n",
    "location_stats.columns = ['loc_mean_salary', 'loc_median_salary', 'loc_std_salary', 'loc_job_count']\n",
    "location_stats = location_stats.reset_index()\n",
    "\n",
    "# Add location features\n",
    "df = df.merge(location_stats, on='company_location', how='left')\n",
    "df['location_market_index'] = df['loc_mean_salary'] / df['loc_std_salary'].replace(0, df['loc_std_salary'].mean())\n",
    "df['location_competitiveness'] = np.log1p(df['loc_job_count'])\n",
    "\n",
    "# 3. Company size and experience encoding with market adjustment\n",
    "size_map = {'S': 1, 'M': 2, 'L': 3}\n",
    "exp_map = {'EN': 1, 'MI': 2, 'SE': 3, 'EX': 4}\n",
    "emp_map = {'FT': 1.0, 'CT': 0.8, 'FL': 0.7, 'PT': 0.5}\n",
    "\n",
    "df['company_size_encoded'] = df['company_size'].map(size_map)\n",
    "df['experience_encoded'] = df['experience_level'].map(exp_map)\n",
    "df['employment_factor'] = df['employment_type'].map(emp_map)\n",
    "\n",
    "# 4. Create advanced composite features\n",
    "df['experience_value'] = df['experience_encoded'] * df['job_value']\n",
    "df['company_value'] = df['company_size_encoded'] * df['location_market_index']\n",
    "df['market_potential'] = df['experience_value'] * df['company_value'] * df['employment_factor']\n",
    "\n",
    "# 5. Time and remote work features\n",
    "df['year_norm'] = (df['work_year'] - df['work_year'].min()) / (df['work_year'].max() - df['work_year'].min())\n",
    "df['remote_norm'] = df['remote_ratio'] / 100\n",
    "df['remote_experience'] = df['remote_norm'] * df['experience_encoded']\n",
    "\n",
    "# 6. Final feature selection\n",
    "features = [\n",
    "    'seniority',\n",
    "    'complexity',\n",
    "    'job_value',\n",
    "    'location_market_index',\n",
    "    'location_competitiveness',\n",
    "    'company_size_encoded',\n",
    "    'experience_encoded',\n",
    "    'employment_factor',\n",
    "    'experience_value',\n",
    "    'company_value',\n",
    "    'market_potential',\n",
    "    'year_norm',\n",
    "    'remote_norm',\n",
    "    'remote_experience'\n",
    "]\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[features]\n",
    "y = df['salary_in_usd']\n",
    "\n",
    "# Log transform the target variable\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=features)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_log, test_size=0.2, random_state=42)\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(X.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: XGBoost Regressor\n",
    "xgb_reg = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=6,\n",
    "    min_child_weight=5,\n",
    "    gamma=0.2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "xgb_pred_log = xgb_reg.predict(X_test)\n",
    "xgb_pred = np.expm1(xgb_pred_log)\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, y_true_log, y_pred_log):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true_log, y_pred_log)  # R² on log scale for better interpretation\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"Root Mean Squared Error: ${rmse:,.2f}\")\n",
    "    print(f\"Mean Absolute Error: ${mae:,.2f}\")\n",
    "    print(f\"R² Score (on log scale): {r2:.4f}\")\n",
    "    print(f\"Mean Absolute Percentage Error: {mape:.2f}%\")\n",
    "\n",
    "print(\"XGBoost Model Performance:\")\n",
    "calculate_metrics(np.expm1(y_test), xgb_pred, y_test, xgb_pred_log)\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': xgb_reg.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Gradient Boosting Regressor\n",
    "gb_reg = GradientBoostingRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=6,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_reg.fit(X_train, y_train)\n",
    "gb_pred_log = gb_reg.predict(X_test)\n",
    "gb_pred = np.expm1(gb_pred_log)\n",
    "\n",
    "print(\"Gradient Boosting Model Performance:\")\n",
    "calculate_metrics(np.expm1(y_test), gb_pred, y_test, gb_pred_log)\n",
    "\n",
    "# Plot feature importance\n",
    "gb_feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': gb_reg.feature_importances_\n",
    "})\n",
    "gb_feature_importance = gb_feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=gb_feature_importance)\n",
    "plt.title('Gradient Boosting Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions\n",
    "def analyze_predictions(y_true, y_pred, model_name):\n",
    "    errors = y_true - y_pred\n",
    "    percentage_errors = (errors / y_true) * 100\n",
    "    \n",
    "    print(f\"\\n{model_name} Prediction Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Error distribution\n",
    "    print(\"Error Distribution:\")\n",
    "    print(f\"Within ±10%: {np.mean(np.abs(percentage_errors) <= 10):.2%}\")\n",
    "    print(f\"Within ±20%: {np.mean(np.abs(percentage_errors) <= 20):.2%}\")\n",
    "    print(f\"Within ±30%: {np.mean(np.abs(percentage_errors) <= 30):.2%}\")\n",
    "    \n",
    "    # Sample predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    indices = np.random.choice(len(y_true), 5, replace=False)\n",
    "    for idx in indices:\n",
    "        print(f\"\\nActual: ${y_true[idx]:,.2f}\")\n",
    "        print(f\"Predicted: ${y_pred[idx]:,.2f}\")\n",
    "        print(f\"Error: {percentage_errors[idx]:+.1f}%\")\n",
    "\n",
    "# Analyze both models\n",
    "analyze_predictions(np.expm1(y_test), xgb_pred, \"XGBoost\")\n",
    "analyze_predictions(np.expm1(y_test), gb_pred, \"Gradient Boosting\")\n",
    "\n",
    "# Plot actual vs predicted\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "ax1.scatter(np.expm1(y_test), xgb_pred, alpha=0.5)\n",
    "ax1.plot([0, np.expm1(y_test).max()], [0, np.expm1(y_test).max()], 'r--')\n",
    "ax1.set_title('XGBoost: Actual vs Predicted')\n",
    "ax1.set_xlabel('Actual Salary')\n",
    "ax1.set_ylabel('Predicted Salary')\n",
    "\n",
    "ax2.scatter(np.expm1(y_test), gb_pred, alpha=0.5)\n",
    "ax2.plot([0, np.expm1(y_test).max()], [0, np.expm1(y_test).max()], 'r--')\n",
    "ax2.set_title('Gradient Boosting: Actual vs Predicted')\n",
    "ax2.set_xlabel('Actual Salary')\n",
    "ax2.set_ylabel('Predicted Salary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
